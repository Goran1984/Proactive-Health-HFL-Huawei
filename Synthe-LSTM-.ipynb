{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97285c9e-727c-444f-99f8-63db40d0826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindspore import nn, Tensor\n",
    "import mindspore.ops as ops\n",
    "import numpy as np\n",
    "\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b21dbacb-bfeb-4064-927d-c08061f7465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore version: 2.7.1.post1\n",
      "Mode set to PYNATIVE for step-by-step teaching.\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "print(\"MindSpore version:\", ms.__version__)\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE)\n",
    "print(\"Mode set to PYNATIVE for step-by-step teaching.\")\n",
    "# https://www.mindspore.cn/docs/en/master/faq/installation.html\n",
    "# https://www.mindspore.cn/install/en/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f05533-84a5-4c0f-a548-4e46f666197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[-0.101678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindspore import nn, Tensor\n",
    "\n",
    "# 1) Define a simple model\n",
    "class SimpleNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Dense(2, 1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "net = SimpleNet()\n",
    "\n",
    "# 2) Create a sample input tensor\n",
    "x = Tensor(np.array([[1.0, 2.0]]), ms.float32)\n",
    "\n",
    "# 3) Forward pass\n",
    "y = net(x)\n",
    "print(\"Output:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90e5315d-d4fb-4988-ab06-9520e0c6d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets generated with cost per medication and saved in the same directory as the script.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import mindspore as ms",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize faker for generating synthetic data\n",
    "fake = Faker()\n",
    "# Set random seeds for reproducibility\n",
    "# ms.set_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "# Define dataset properties\n",
    "medication_names = [f\"Medication_{i}\" for i in range(1, 31)]\n",
    "atc_codes = [f\"ATC_{i}\" for i in range(1, 31)]\n",
    "diagnosis_codes = [f\"D_{i}\" for i in range(1, 21)]\n",
    "prescribers = [f\"Prescriber_{i}\" for i in range(1, 11)]\n",
    "\n",
    "# Get a date range within the past month\n",
    "today = datetime.now()\n",
    "one_month_ago = today - timedelta(days=30)\n",
    "\n",
    "def generate_pharmacy_data(pharmacy_id, num_records):\n",
    "    data = []\n",
    "    for _ in range(num_records):\n",
    "        transaction_id = fake.uuid4()\n",
    "        age = random.randint(1, 100)\n",
    "        num_medications = random.randint(1, 5)\n",
    "        medications = random.sample(medication_names, num_medications)\n",
    "        atc_codes_for_medications = random.choices(atc_codes, k=num_medications)\n",
    "        dosages = [random.randint(1, 500) for _ in range(num_medications)]\n",
    "        quantities = [random.randint(1, 10) for _ in range(num_medications)]\n",
    "        costs_per_medication = [round(random.uniform(5, 50), 2) for _ in range(num_medications)]\n",
    "        diagnosis_code = random.choice(diagnosis_codes)\n",
    "        prescriber_id = random.choice(prescribers)\n",
    "        date = (one_month_ago + timedelta(days=random.randint(0, 30))).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        for medication, atc_code, dosage, quantity, cost_per_medication in zip(\n",
    "            medications, atc_codes_for_medications, dosages, quantities, costs_per_medication\n",
    "        ):\n",
    "            data.append([\n",
    "                transaction_id, age, medication, atc_code, dosage, quantity,\n",
    "                cost_per_medication, diagnosis_code, prescriber_id, date\n",
    "            ])\n",
    "    return data\n",
    "\n",
    "# Define dataset structure\n",
    "fields = [\n",
    "    \"Transaction_ID\", \"Age\", \"Medication_Name\", \"ATC_Code\", \"Dosage\", \n",
    "    \"Quantity\", \"Cost_Per_Medication\", \"Diagnosis_Code\", \"Prescriber_ID\", \"Date\"\n",
    "]\n",
    "\n",
    "# Generate datasets\n",
    "def generate_datasets():\n",
    "    for city in range(1, 4):  # 3 cities\n",
    "        for zone in range(1, 4):  # 3 zones per city\n",
    "            for pharmacy in range(1, 5):  # 4 pharmacies per zone\n",
    "                dataset_name = f\"Ph{pharmacy:02d}_Z{zone:02d}_C{city:02d}\"\n",
    "                # Generate training dataset\n",
    "                training_data = generate_pharmacy_data(dataset_name, 500)\n",
    "                train_df = pd.DataFrame(training_data, columns=fields)\n",
    "                train_df.to_csv(f\"{dataset_name}_train.csv\", index=False)\n",
    "\n",
    "                # Generate testing dataset\n",
    "                testing_data = generate_pharmacy_data(dataset_name, 200)\n",
    "                test_df = pd.DataFrame(testing_data, columns=fields)\n",
    "                #test_df.to_csv(f\"{dataset_name}_test.csv\", index=False)\n",
    "\n",
    "generate_datasets()\n",
    "print(\"Datasets generated with cost per medication and saved in the same directory as the script.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe474e41-fb23-4c5a-b91f-9b01f6b45fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TEST dataset with 200 transactions has been generated and saved as TEST.csv.\n"
     ]
    }
   ],
   "source": [
    "def generate_single_dataset(dataset_name, num_records):\n",
    "    data = generate_pharmacy_data(dataset_name, num_records)\n",
    "    df = pd.DataFrame(data, columns=fields)\n",
    "    df.to_csv(f\"{dataset_name}.csv\", index=False)\n",
    "\n",
    "# Generate the TEST dataset\n",
    "generate_single_dataset(\"TEST\", 200)\n",
    "\n",
    "print(\"The TEST dataset with 200 transactions has been generated and saved as TEST.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8f7e5c0-47a1-4354-8e08-fe381ae9073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for ./Ph01_Z01_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0090\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z01_C01_train_model.ckpt\n",
      "Training model for ./Ph01_Z01_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0108\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z01_C02_train_model.ckpt\n",
      "Training model for ./Ph01_Z01_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0086\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z01_C03_train_model.ckpt\n",
      "Training model for ./Ph01_Z02_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0088\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z02_C01_train_model.ckpt\n",
      "Training model for ./Ph01_Z02_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0085\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z02_C02_train_model.ckpt\n",
      "Training model for ./Ph01_Z02_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0100\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z02_C03_train_model.ckpt\n",
      "Training model for ./Ph01_Z03_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0096\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z03_C01_train_model.ckpt\n",
      "Training model for ./Ph01_Z03_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0097\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z03_C02_train_model.ckpt\n",
      "Training model for ./Ph01_Z03_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0085\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph01_Z03_C03_train_model.ckpt\n",
      "Training model for ./Ph02_Z01_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0095\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph02_Z01_C01_train_model.ckpt\n",
      "Training model for ./Ph02_Z01_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0094\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph02_Z01_C02_train_model.ckpt\n",
      "Training model for ./Ph02_Z01_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0085\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph02_Z01_C03_train_model.ckpt\n",
      "Training model for ./Ph02_Z02_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0099\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph02_Z02_C01_train_model.ckpt\n",
      "Training model for ./Ph02_Z02_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0080\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph02_Z02_C02_train_model.ckpt\n",
      "Training model for ./Ph02_Z02_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0080\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph02_Z02_C03_train_model.ckpt\n",
      "Training model for ./Ph02_Z03_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0068\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph02_Z03_C01_train_model.ckpt\n",
      "Training model for ./Ph02_Z03_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0084\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph02_Z03_C02_train_model.ckpt\n",
      "Training model for ./Ph02_Z03_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0115\n",
      "Epoch 2/2, Loss: 0.0002\n",
      "Model saved to ./output/Ph02_Z03_C03_train_model.ckpt\n",
      "Training model for ./Ph03_Z01_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0086\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph03_Z01_C01_train_model.ckpt\n",
      "Training model for ./Ph03_Z01_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0099\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph03_Z01_C02_train_model.ckpt\n",
      "Training model for ./Ph03_Z01_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0101\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph03_Z01_C03_train_model.ckpt\n",
      "Training model for ./Ph03_Z02_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0076\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph03_Z02_C01_train_model.ckpt\n",
      "Training model for ./Ph03_Z02_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0069\n",
      "Epoch 2/2, Loss: 0.0000\n",
      "Model saved to ./output/Ph03_Z02_C02_train_model.ckpt\n",
      "Training model for ./Ph03_Z02_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0116\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph03_Z02_C03_train_model.ckpt\n",
      "Training model for ./Ph03_Z03_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0080\n",
      "Epoch 2/2, Loss: 0.0000\n",
      "Model saved to ./output/Ph03_Z03_C01_train_model.ckpt\n",
      "Training model for ./Ph03_Z03_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0102\n",
      "Epoch 2/2, Loss: 0.0002\n",
      "Model saved to ./output/Ph03_Z03_C02_train_model.ckpt\n",
      "Training model for ./Ph03_Z03_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0077\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph03_Z03_C03_train_model.ckpt\n",
      "Training model for ./Ph04_Z01_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0105\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph04_Z01_C01_train_model.ckpt\n",
      "Training model for ./Ph04_Z01_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0122\n",
      "Epoch 2/2, Loss: 0.0002\n",
      "Model saved to ./output/Ph04_Z01_C02_train_model.ckpt\n",
      "Training model for ./Ph04_Z01_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0099\n",
      "Epoch 2/2, Loss: 0.0002\n",
      "Model saved to ./output/Ph04_Z01_C03_train_model.ckpt\n",
      "Training model for ./Ph04_Z02_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0089\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph04_Z02_C01_train_model.ckpt\n",
      "Training model for ./Ph04_Z02_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0066\n",
      "Epoch 2/2, Loss: 0.0000\n",
      "Model saved to ./output/Ph04_Z02_C02_train_model.ckpt\n",
      "Training model for ./Ph04_Z02_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0082\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph04_Z02_C03_train_model.ckpt\n",
      "Training model for ./Ph04_Z03_C01_train.csv\n",
      "Epoch 1/2, Loss: 0.0091\n",
      "Epoch 2/2, Loss: 0.0002\n",
      "Model saved to ./output/Ph04_Z03_C01_train_model.ckpt\n",
      "Training model for ./Ph04_Z03_C02_train.csv\n",
      "Epoch 1/2, Loss: 0.0113\n",
      "Epoch 2/2, Loss: 0.0001\n",
      "Model saved to ./output/Ph04_Z03_C02_train_model.ckpt\n",
      "Training model for ./Ph04_Z03_C03_train.csv\n",
      "Epoch 1/2, Loss: 0.0074\n",
      "Epoch 2/2, Loss: 0.0000\n",
      "Model saved to ./output/Ph04_Z03_C03_train_model.ckpt\n",
      "City and national aggregation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import nn, Tensor, ops\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "ms.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE)\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM Model (MindSpore)\n",
    "# -----------------------------\n",
    "class LSTMModel(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Dense(hidden_size, output_size)\n",
    "\n",
    "    def construct(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Generator\n",
    "# -----------------------------\n",
    "class PharmacyDataset:\n",
    "    def __init__(self, data, features, target):\n",
    "        self.X = data[features].values.astype(np.float32)\n",
    "        self.y = data[target].values.astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index].reshape(1, 1, -1)  # âœ… FIXED: add batch + sequence dim\n",
    "        y = self.y[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load & preprocess data\n",
    "# -----------------------------\n",
    "def load_data(file_path, scaler=None):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    data['Medication_Cost'] = data['Cost_Per_Medication'] * data['Quantity']\n",
    "\n",
    "    features = ['Age', 'Dosage', 'Quantity', 'Medication_Cost']\n",
    "    target = 'Medication_Cost'\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        data[features] = scaler.fit_transform(data[features])\n",
    "    else:\n",
    "        data[features] = scaler.transform(data[features])\n",
    "\n",
    "    return data, features, target, scaler\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "def train_model(model, dataset, optimizer, loss_fn, epochs):\n",
    "    model.set_train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataset:\n",
    "            x = Tensor(x)\n",
    "            y = Tensor(y)\n",
    "\n",
    "            def forward_fn(x, y):\n",
    "                preds = model(x)\n",
    "                loss = loss_fn(preds.squeeze(), y)\n",
    "                return loss\n",
    "\n",
    "            grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "            loss, grads = grad_fn(x, y)\n",
    "            optimizer(grads)\n",
    "\n",
    "            total_loss += loss.asnumpy()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataset):.4f}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction\n",
    "# -----------------------------\n",
    "def predict_model(model, dataset):\n",
    "    model.set_train(False)\n",
    "    preds = []\n",
    "    for x, _ in dataset:\n",
    "        x = Tensor(x)\n",
    "        out = model(x)\n",
    "        preds.append(out.asnumpy().item())\n",
    "    return preds\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Save / Load model\n",
    "# -----------------------------\n",
    "def save_model(model, path):\n",
    "    ms.save_checkpoint(model, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    ms.load_checkpoint(path, net=model)\n",
    "    model.set_train(False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train pharmacy models\n",
    "# -----------------------------\n",
    "def train_pharmacy_models(data_paths, output_path, scaler=None):\n",
    "    pharmacy_models = []\n",
    "\n",
    "    for file_path in data_paths:\n",
    "        data, features, target, scaler = load_data(file_path, scaler)\n",
    "        dataset = GeneratorDataset(\n",
    "            PharmacyDataset(data, features, target),\n",
    "            column_names=[\"x\", \"y\"],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        model = LSTMModel(len(features), 64, 2, 1)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = nn.Adam(model.trainable_params(), learning_rate=0.001)\n",
    "\n",
    "        print(f\"Training model for {file_path}\")\n",
    "        train_model(model, dataset, optimizer, loss_fn, epochs=2)\n",
    "\n",
    "        model_path = os.path.join(\n",
    "            output_path, f\"{os.path.basename(file_path).split('.')[0]}_model.ckpt\"\n",
    "        )\n",
    "        save_model(model, model_path)\n",
    "\n",
    "        pharmacy_models.append(model)\n",
    "\n",
    "    return pharmacy_models\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregation logic (UNCHANGED)\n",
    "# -----------------------------\n",
    "def aggregate_pharmacy_models(pharmacy_models, data_paths, output_path):\n",
    "    city_predictions = {f\"City{i}\": [] for i in range(1, 4)}\n",
    "\n",
    "    for idx, file_path in enumerate(data_paths):\n",
    "        data, features, target, _ = load_data(file_path)\n",
    "        dataset = GeneratorDataset(\n",
    "            PharmacyDataset(data, features, target),\n",
    "            column_names=[\"x\", \"y\"],\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        preds = predict_model(pharmacy_models[idx], dataset)\n",
    "        data['Predicted_Cost'] = preds\n",
    "        data['Predicted_Trend_Change'] = np.random.randn(len(preds))\n",
    "\n",
    "        city_id = file_path.split(\"_C\")[1][:2]\n",
    "        city_predictions[f\"City{int(city_id)}\"].append(\n",
    "            data[['Medication_Name', 'Predicted_Cost', 'Predicted_Trend_Change']]\n",
    "        )\n",
    "\n",
    "    for city, blocks in city_predictions.items():\n",
    "        df = pd.concat(blocks)\n",
    "        df = df.groupby('Medication_Name').mean().reset_index()\n",
    "        df.to_csv(os.path.join(output_path, f\"{city}_predictions.csv\"), index=False)\n",
    "        city_predictions[city] = df\n",
    "\n",
    "    return city_predictions\n",
    "\n",
    "\n",
    "def aggregate_city_models(city_predictions, output_path):\n",
    "    national = pd.concat(city_predictions.values())\n",
    "    national = national.groupby('Medication_Name').mean().reset_index()\n",
    "    national.to_csv(os.path.join(output_path, \"national_aggregated_predictions.csv\"), index=False)\n",
    "    return national\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = \"./\"\n",
    "    output_path = \"./output/\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    pharmacy_data_paths = [\n",
    "        os.path.join(data_path, f\"Ph{p:02d}_Z{z:02d}_C{c:02d}_train.csv\")\n",
    "        for p in range(1, 5) for z in range(1, 4) for c in range(1, 4)\n",
    "    ]\n",
    "\n",
    "    pharmacy_models = train_pharmacy_models(pharmacy_data_paths, output_path)\n",
    "    city_predictions = aggregate_pharmacy_models(pharmacy_models, pharmacy_data_paths, output_path)\n",
    "    national_predictions = aggregate_city_models(city_predictions, output_path)\n",
    "\n",
    "    print(\"City and national aggregation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7f089-65ad-4395-a4ab-499397a719b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MindSpore msenv)",
   "language": "python",
   "name": "msenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
