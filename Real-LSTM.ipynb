{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7f089-65ad-4395-a4ab-499397a719b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Zone 1 in City1...\n",
      "Epoch 1/2, Loss: 0.0213\n",
      "Epoch 2/2, Loss: 0.0199\n",
      "Training Zone 2 in City1...\n",
      "Epoch 1/2, Loss: 0.0199\n",
      "Epoch 2/2, Loss: 0.0191\n",
      "Training Zone 3 in City1...\n",
      "Epoch 1/2, Loss: 0.0210\n",
      "Epoch 2/2, Loss: 0.0203\n",
      "Training Zone 1 in City2...\n",
      "Epoch 1/2, Loss: 0.0238\n",
      "Epoch 2/2, Loss: 0.0229\n",
      "Training Zone 2 in City2...\n",
      "Epoch 1/2, Loss: 0.0199\n",
      "Epoch 2/2, Loss: 0.0194\n",
      "Training Zone 3 in City2...\n",
      "Epoch 1/2, Loss: 0.0206\n",
      "Epoch 2/2, Loss: 0.0197\n",
      "Training Zone 1 in City3...\n",
      "Epoch 1/2, Loss: 0.0306\n",
      "Epoch 2/2, Loss: 0.0301\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import nn, Tensor\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "# Set random seeds\n",
    "ms.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE)\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM Model\n",
    "# -----------------------------\n",
    "class LSTMModel(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Dense(hidden_size, output_size)\n",
    "\n",
    "        # Save architecture info for aggregation (same role as PyTorch attributes)\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._output_size = output_size\n",
    "\n",
    "    def construct(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset (MindSpore Generator)\n",
    "# -----------------------------\n",
    "class PharmacyDataset:\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.array(self.sequences[idx], dtype=np.float32)  # (seq_len, features)\n",
    "        y = np.array(self.targets[idx], dtype=np.float32)    # (features,)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load Data\n",
    "# -----------------------------\n",
    "def load_data(file_path, global_columns=None):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    data['addeddate'] = pd.to_datetime(\n",
    "        data['addeddate'].astype(str).str.split().str[0],\n",
    "        errors='coerce'\n",
    "    )\n",
    "    data.dropna(subset=['addeddate'], inplace=True)\n",
    "\n",
    "    data['month'] = data['addeddate'].dt.to_period('M')\n",
    "    data['medication_combination'] = data.groupby('Invoice')['name'].transform(\n",
    "        lambda x: '+'.join(sorted(set(x)))\n",
    "    )\n",
    "\n",
    "    grouped_data = data.groupby(['month', 'medication_combination']).size().unstack(fill_value=0)\n",
    "\n",
    "    if global_columns is not None:\n",
    "        grouped_data = grouped_data.reindex(columns=global_columns, fill_value=0)\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Create Sequences\n",
    "# -----------------------------\n",
    "def create_sequences(data, seq_length=3):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data.iloc[i:i + seq_length].values)\n",
    "        targets.append(data.iloc[i + seq_length].values)\n",
    "    return sequences, targets\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train Model\n",
    "# -----------------------------\n",
    "def train_model(model, dataset, loss_fn, optimizer, epochs=5):\n",
    "    model.set_train(True)\n",
    "\n",
    "    def forward_fn(x, y):\n",
    "        preds = model(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        for x_batch, y_batch in dataset.create_tuple_iterator():\n",
    "            x_batch = Tensor(x_batch, ms.float32)  # (batch, seq_len, features)\n",
    "            y_batch = Tensor(y_batch, ms.float32)  # (batch, features)\n",
    "\n",
    "            loss, grads = grad_fn(x_batch, y_batch)\n",
    "            optimizer(grads)\n",
    "\n",
    "            epoch_loss += float(loss.asnumpy())\n",
    "            steps += 1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / max(steps, 1):.4f}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregate Models (Average weights)\n",
    "# -----------------------------\n",
    "def aggregate_models(models):\n",
    "    if not models:\n",
    "        return None\n",
    "\n",
    "    input_size = models[0]._input_size\n",
    "    hidden_size = models[0]._hidden_size\n",
    "    num_layers = models[0]._num_layers\n",
    "    output_size = models[0]._output_size\n",
    "\n",
    "    agg_model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "    # Collect params as numpy\n",
    "    param_lists = []\n",
    "    for m in models:\n",
    "        params = {p.name: p.data.asnumpy() for p in m.get_parameters()}\n",
    "        param_lists.append(params)\n",
    "\n",
    "    # Average\n",
    "    avg_params = {}\n",
    "    for name in param_lists[0].keys():\n",
    "        avg_params[name] = np.mean([pl[name] for pl in param_lists], axis=0)\n",
    "\n",
    "    # Load averaged weights\n",
    "    for p in agg_model.get_parameters():\n",
    "        if p.name in avg_params:\n",
    "            p.set_data(Tensor(avg_params[p.name], dtype=p.data.dtype))\n",
    "\n",
    "    return agg_model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Predict Trends\n",
    "# -----------------------------\n",
    "def predict_trends(model, data, seq_length=3, steps=3):\n",
    "    model.set_train(False)\n",
    "\n",
    "    inputs = Tensor(data.iloc[-seq_length:].values.astype(np.float32), ms.float32)\n",
    "    inputs = inputs.expand_dims(0)  # (1, seq_len, features)\n",
    "\n",
    "    predictions = []\n",
    "    for _ in range(steps):\n",
    "        pred = model(inputs).asnumpy()  # (1, features)\n",
    "        predictions.append(pred)\n",
    "\n",
    "        pred_tensor = Tensor(pred.astype(np.float32), ms.float32).expand_dims(1)  # (1, 1, features)\n",
    "        inputs = ms.ops.concat((inputs[:, 1:, :], pred_tensor), axis=1)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Save Predictions (to Real_LSTM_Result)\n",
    "# -----------------------------\n",
    "def save_predictions(level_models, level_data, level, result_folder):\n",
    "    if level_models[level] is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(level_models[level], list):\n",
    "        for i, model in enumerate(level_models[level]):\n",
    "            if level not in level_data or i >= len(level_data[level]):\n",
    "                continue\n",
    "\n",
    "            predictions = predict_trends(model, level_data[level][i])\n",
    "            all_medications = level_data[level][i].columns\n",
    "            pred_df = pd.DataFrame(predictions.squeeze(), columns=all_medications)\n",
    "\n",
    "            output_path = os.path.join(result_folder, f\"{level}_predictions_{i}.csv\")\n",
    "            pred_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved {level} predictions → {output_path}\")\n",
    "    else:\n",
    "        if level not in level_data or level_data[level] is None:\n",
    "            return\n",
    "\n",
    "        predictions = predict_trends(level_models[level], level_data[level])\n",
    "        all_medications = level_data[level].columns\n",
    "        pred_df = pd.DataFrame(predictions.squeeze(), columns=all_medications)\n",
    "\n",
    "        output_path = os.path.join(result_folder, f\"{level}_predictions.csv\")\n",
    "        pred_df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved {level} predictions → {output_path}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main Execution\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    output_folder = \"./outputDateUni/\"   # INPUT real dataset folder (unchanged)\n",
    "    cities = [\"City1\", \"City2\", \"City3\"]\n",
    "    zones_per_city = 3\n",
    "    pharmacies_per_zone = 4\n",
    "    seq_length = 3\n",
    "\n",
    "    # OUTPUT folder for results (NEW)\n",
    "    result_folder = \"./Real_LSTM_Result/\"\n",
    "    os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "    level_models = {'pharmacy': [], 'zone': [], 'city': [], 'national': None}\n",
    "    level_data = {'zone': [], 'city': [], 'national': None}\n",
    "\n",
    "    # Step 1: Collect all medication combinations to create a global column set\n",
    "    all_columns = set()\n",
    "    for city in cities:\n",
    "        for zone in range(1, zones_per_city + 1):\n",
    "            for pharmacy in range(1, pharmacies_per_zone + 1):\n",
    "                pharmacy_path = os.path.join(\n",
    "                    output_folder, city, f\"Zone{zone}\",\n",
    "                    f\"Ph{pharmacy:02d}_Z{zone:02d}_C{cities.index(city) + 1:02d}.csv\"\n",
    "                )\n",
    "                if os.path.exists(pharmacy_path):\n",
    "                    data = pd.read_csv(pharmacy_path)\n",
    "                    data['medication_combination'] = data.groupby('Invoice')['name'].transform(\n",
    "                        lambda x: '+'.join(sorted(set(x)))\n",
    "                    )\n",
    "                    all_columns.update(data['medication_combination'].unique())\n",
    "\n",
    "    global_columns = sorted(all_columns)\n",
    "\n",
    "    # Step 2: Train models with aligned columns (Zone-level training)\n",
    "    for city in cities:\n",
    "        for zone in range(1, zones_per_city + 1):\n",
    "            zone_data = []\n",
    "\n",
    "            for pharmacy in range(1, pharmacies_per_zone + 1):\n",
    "                pharmacy_path = os.path.join(\n",
    "                    output_folder, city, f\"Zone{zone}\",\n",
    "                    f\"Ph{pharmacy:02d}_Z{zone:02d}_C{cities.index(city) + 1:02d}.csv\"\n",
    "                )\n",
    "                if os.path.exists(pharmacy_path):\n",
    "                    df = load_data(pharmacy_path, global_columns=global_columns)\n",
    "                    zone_data.append(df)\n",
    "\n",
    "            if not zone_data:\n",
    "                continue\n",
    "\n",
    "            merged_data = sum(zone_data) / len(zone_data)\n",
    "            sequences, targets = create_sequences(merged_data, seq_length)\n",
    "            if not sequences:\n",
    "                continue\n",
    "\n",
    "            dataset_obj = PharmacyDataset(sequences, targets)\n",
    "            dataset = GeneratorDataset(dataset_obj, column_names=[\"x\", \"y\"], shuffle=True).batch(32)\n",
    "\n",
    "            model = LSTMModel(\n",
    "                input_size=merged_data.shape[1],\n",
    "                hidden_size=64,\n",
    "                num_layers=2,\n",
    "                output_size=merged_data.shape[1]\n",
    "            )\n",
    "\n",
    "            optimizer = nn.Adam(model.trainable_params(), learning_rate=0.001)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            print(f\"Training Zone {zone} in {city}...\")\n",
    "            train_model(model, dataset, criterion, optimizer, epochs=2)\n",
    "\n",
    "            level_models['zone'].append(model)\n",
    "            level_data['zone'].append(merged_data)\n",
    "\n",
    "    # Step 3: Aggregate models at the city level\n",
    "    for i in range(0, len(level_models['zone']), 3):\n",
    "        city_models = level_models['zone'][i:i + 3]\n",
    "        city_data = level_data['zone'][i:i + 3]\n",
    "        if city_models:\n",
    "            level_models['city'].append(aggregate_models(city_models))\n",
    "            level_data['city'].append(sum(city_data) / len(city_data))\n",
    "\n",
    "    # Step 4: Aggregate models at the national level\n",
    "    if level_models['city']:\n",
    "        level_models['national'] = aggregate_models(level_models['city'])\n",
    "        level_data['national'] = sum(level_data['city']) / len(level_data['city'])\n",
    "\n",
    "    # Step 5: Save predictions (to Real_LSTM_Result)\n",
    "    for level in level_models:\n",
    "        save_predictions(level_models, level_data, level, result_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7bcba3-78a3-4547-97d9-cd2210e60fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing City1 ===\n",
      "Training City1 Zone 1\n",
      "Epoch 1/2, Loss: 0.0213\n",
      "Epoch 2/2, Loss: 0.0199\n",
      "Training City1 Zone 2\n",
      "Epoch 1/2, Loss: 0.0199\n",
      "Epoch 2/2, Loss: 0.0191\n",
      "Training City1 Zone 3\n",
      "Epoch 1/2, Loss: 0.0210\n",
      "Epoch 2/2, Loss: 0.0203\n",
      "✔ City predictions saved → ./Real_LSTM_Result/City1_predictions.csv\n",
      "\n",
      "=== Processing City2 ===\n",
      "Training City2 Zone 1\n",
      "Epoch 1/2, Loss: 0.0236\n",
      "Epoch 2/2, Loss: 0.0229\n",
      "Training City2 Zone 2\n",
      "Epoch 1/2, Loss: 0.0199\n",
      "Epoch 2/2, Loss: 0.0192\n",
      "Training City2 Zone 3\n",
      "Epoch 1/2, Loss: 0.0205\n",
      "Epoch 2/2, Loss: 0.0198\n",
      "✔ City predictions saved → ./Real_LSTM_Result/City2_predictions.csv\n",
      "\n",
      "=== Processing City3 ===\n",
      "Training City3 Zone 1\n",
      "Epoch 1/2, Loss: 0.0307\n",
      "Epoch 2/2, Loss: 0.0300\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 833. MiB for an array with shape (256, 426247) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 204\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    199\u001b[0m dataset \u001b[38;5;241m=\u001b[39m GeneratorDataset(\n\u001b[0;32m    200\u001b[0m     PharmacyDataset(X, y),\n\u001b[0;32m    201\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    202\u001b[0m )\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m--> 204\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLSTMModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m opt \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mtrainable_params(), learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m    209\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m, in \u001b[0;36mLSTMModel.__init__\u001b[1;34m(self, input_size, hidden_size, num_layers, output_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size, num_layers, output_size):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDense(hidden_size, output_size)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_size \u001b[38;5;241m=\u001b[39m input_size\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msenv\\lib\\site-packages\\mindspore\\nn\\layer\\rnns.py:850\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    849\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 850\u001b[0m     \u001b[38;5;28msuper\u001b[39m(LSTM, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(mode, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msenv\\lib\\site-packages\\mindspore\\nn\\layer\\rnns.py:439\u001b[0m, in \u001b[0;36m_RNNBase.__init__\u001b[1;34m(self, mode, input_size, hidden_size, num_layers, has_bias, batch_first, dropout, bidirectional, dtype)\u001b[0m\n\u001b[0;32m    435\u001b[0m layer_input_size \u001b[38;5;241m=\u001b[39m input_size \u001b[38;5;28;01mif\u001b[39;00m layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m hidden_size \u001b[38;5;241m*\u001b[39m num_directions\n\u001b[0;32m    436\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_reverse\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_ih_list\u001b[38;5;241m.\u001b[39mappend(Parameter(\n\u001b[1;32m--> 439\u001b[0m     Tensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstdv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mgate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_input_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m    440\u001b[0m            dtype\u001b[38;5;241m=\u001b[39mdtype), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_ih_l\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(layer, suffix)))\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_hh_list\u001b[38;5;241m.\u001b[39mappend(Parameter(\n\u001b[0;32m    442\u001b[0m     Tensor(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39mstdv, stdv, (gate_size, hidden_size))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m    443\u001b[0m            dtype\u001b[38;5;241m=\u001b[39mdtype), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_hh_l\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(layer, suffix)))\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_bias:\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:1158\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.uniform\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:636\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 833. MiB for an array with shape (256, 426247) and data type float64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import nn, Tensor\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "# -----------------------------\n",
    "# Setup\n",
    "# -----------------------------\n",
    "ms.set_seed(42)\n",
    "np.random.seed(42)\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE)\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM Model\n",
    "# -----------------------------\n",
    "class LSTMModel(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Dense(hidden_size, output_size)\n",
    "\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._output_size = output_size\n",
    "\n",
    "    def construct(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class PharmacyDataset:\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            np.array(self.sequences[idx], dtype=np.float32),\n",
    "            np.array(self.targets[idx], dtype=np.float32)\n",
    "        )\n",
    "\n",
    "# -----------------------------\n",
    "# Load Data\n",
    "# -----------------------------\n",
    "def load_data(file_path, global_columns=None):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    data['addeddate'] = pd.to_datetime(\n",
    "        data['addeddate'].astype(str).str.split().str[0],\n",
    "        errors='coerce'\n",
    "    )\n",
    "    data.dropna(subset=['addeddate'], inplace=True)\n",
    "\n",
    "    data['month'] = data['addeddate'].dt.to_period('M')\n",
    "    data['medication_combination'] = data.groupby('Invoice')['name'].transform(\n",
    "        lambda x: '+'.join(sorted(set(x)))\n",
    "    )\n",
    "\n",
    "    grouped = data.groupby(['month', 'medication_combination']).size().unstack(fill_value=0)\n",
    "\n",
    "    if global_columns is not None:\n",
    "        grouped = grouped.reindex(columns=global_columns, fill_value=0)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# -----------------------------\n",
    "# Create sequences\n",
    "# -----------------------------\n",
    "def create_sequences(data, seq_length=3):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data.iloc[i:i + seq_length].values)\n",
    "        y.append(data.iloc[i + seq_length].values)\n",
    "    return X, y\n",
    "\n",
    "# -----------------------------\n",
    "# Train model\n",
    "# -----------------------------\n",
    "def train_model(model, dataset, loss_fn, optimizer, epochs=2):\n",
    "    model.set_train(True)\n",
    "\n",
    "    def forward_fn(x, y):\n",
    "        return loss_fn(model(x), y)\n",
    "\n",
    "    grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        total, steps = 0.0, 0\n",
    "        for xb, yb in dataset.create_tuple_iterator():\n",
    "            xb, yb = Tensor(xb, ms.float32), Tensor(yb, ms.float32)\n",
    "            loss, grads = grad_fn(xb, yb)\n",
    "            optimizer(grads)\n",
    "            total += float(loss.asnumpy())\n",
    "            steps += 1\n",
    "        print(f\"Epoch {e+1}/{epochs}, Loss: {total/max(steps,1):.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregate models (average)\n",
    "# -----------------------------\n",
    "def aggregate_models(models):\n",
    "    base = models[0]\n",
    "    agg = LSTMModel(\n",
    "        base._input_size, base._hidden_size,\n",
    "        base._num_layers, base._output_size\n",
    "    )\n",
    "\n",
    "    param_sets = [{p.name: p.data.asnumpy() for p in m.get_parameters()} for m in models]\n",
    "\n",
    "    for p in agg.get_parameters():\n",
    "        p.set_data(\n",
    "            Tensor(np.mean([ps[p.name] for ps in param_sets], axis=0), p.data.dtype)\n",
    "        )\n",
    "\n",
    "    return agg\n",
    "\n",
    "# -----------------------------\n",
    "# Predict\n",
    "# -----------------------------\n",
    "def predict_trends(model, data, seq_length=3, steps=3):\n",
    "    model.set_train(False)\n",
    "    x = Tensor(data.iloc[-seq_length:].values.astype(np.float32)).expand_dims(0)\n",
    "\n",
    "    preds = []\n",
    "    for _ in range(steps):\n",
    "        out = model(x).asnumpy()\n",
    "        preds.append(out)\n",
    "        x = ms.ops.concat((x[:, 1:, :], Tensor(out).expand_dims(1)), axis=1)\n",
    "\n",
    "    return np.array(preds)\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_root = \"./outputDateUni/\"\n",
    "    result_root = \"./Real_LSTM_Result/\"\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "\n",
    "    cities = [\"City1\", \"City2\", \"City3\"]\n",
    "    zones_per_city = 3\n",
    "    pharmacies_per_zone = 4\n",
    "    seq_length = 3\n",
    "\n",
    "    # -----------------------------\n",
    "    # Global columns\n",
    "    # -----------------------------\n",
    "    all_cols = set()\n",
    "    for city in cities:\n",
    "        for z in range(1, zones_per_city + 1):\n",
    "            for p in range(1, pharmacies_per_zone + 1):\n",
    "                path = os.path.join(\n",
    "                    input_root, city, f\"Zone{z}\",\n",
    "                    f\"Ph{p:02d}_Z{z:02d}_C{cities.index(city)+1:02d}.csv\"\n",
    "                )\n",
    "                if os.path.exists(path):\n",
    "                    d = pd.read_csv(path)\n",
    "                    d['medication_combination'] = d.groupby('Invoice')['name'].transform(\n",
    "                        lambda x: '+'.join(sorted(set(x)))\n",
    "                    )\n",
    "                    all_cols.update(d['medication_combination'].unique())\n",
    "\n",
    "    global_columns = sorted(all_cols)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Train zones & aggregate per city\n",
    "    # -----------------------------\n",
    "    for city in cities:\n",
    "        print(f\"\\n=== Processing {city} ===\")\n",
    "        zone_models = []\n",
    "        zone_data = []\n",
    "\n",
    "        for z in range(1, zones_per_city + 1):\n",
    "            dfs = []\n",
    "            for p in range(1, pharmacies_per_zone + 1):\n",
    "                path = os.path.join(\n",
    "                    input_root, city, f\"Zone{z}\",\n",
    "                    f\"Ph{p:02d}_Z{z:02d}_C{cities.index(city)+1:02d}.csv\"\n",
    "                )\n",
    "                if os.path.exists(path):\n",
    "                    dfs.append(load_data(path, global_columns))\n",
    "\n",
    "            if not dfs:\n",
    "                continue\n",
    "\n",
    "            merged = sum(dfs) / len(dfs)\n",
    "            X, y = create_sequences(merged, seq_length)\n",
    "            if not X:\n",
    "                continue\n",
    "\n",
    "            dataset = GeneratorDataset(\n",
    "                PharmacyDataset(X, y),\n",
    "                [\"x\", \"y\"], shuffle=True\n",
    "            ).batch(32)\n",
    "\n",
    "            model = LSTMModel(\n",
    "                merged.shape[1], 64, 2, merged.shape[1]\n",
    "            )\n",
    "\n",
    "            opt = nn.Adam(model.trainable_params(), learning_rate=0.001)\n",
    "            loss = nn.MSELoss()\n",
    "\n",
    "            print(f\"Training {city} Zone {z}\")\n",
    "            train_model(model, dataset, loss, opt)\n",
    "\n",
    "            zone_models.append(model)\n",
    "            zone_data.append(merged)\n",
    "\n",
    "        # -----------------------------\n",
    "        # City aggregation\n",
    "        # -----------------------------\n",
    "        if zone_models:\n",
    "            city_model = aggregate_models(zone_models)\n",
    "            city_data = sum(zone_data) / len(zone_data)\n",
    "\n",
    "            preds = predict_trends(city_model, city_data)\n",
    "            df = pd.DataFrame(preds.squeeze(), columns=city_data.columns)\n",
    "\n",
    "            out_path = os.path.join(result_root, f\"{city}_predictions.csv\")\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "            print(f\"✔ City predictions saved → {out_path}\")\n",
    "\n",
    "    print(\"\\n✅ Finished: City-level HFL predictions generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba3a94b-e13d-4848-a62a-44d4c9f8c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Top-500 global columns (this prevents feature explosion)...\n",
      "Using global feature size = 500 columns\n",
      "\n",
      "=== Processing City1 ===\n",
      "Training City1 Zone 1 (features=500)\n",
      "Epoch 1/2, Loss: 2.3103\n",
      "Epoch 2/2, Loss: 2.2883\n",
      "Training City1 Zone 2 (features=500)\n",
      "Epoch 1/2, Loss: 2.2913\n",
      "Epoch 2/2, Loss: 2.2625\n",
      "Training City1 Zone 3 (features=500)\n",
      "Epoch 1/2, Loss: 2.2660\n",
      "Epoch 2/2, Loss: 2.2396\n",
      "✔ City predictions saved → ./Real_LSTM_Result/City1_predictions.csv\n",
      "\n",
      "=== Processing City2 ===\n",
      "Training City2 Zone 1 (features=500)\n",
      "Epoch 1/2, Loss: 2.2931\n",
      "Epoch 2/2, Loss: 2.2672\n",
      "Training City2 Zone 2 (features=500)\n",
      "Epoch 1/2, Loss: 2.2232\n",
      "Epoch 2/2, Loss: 2.1972\n",
      "Training City2 Zone 3 (features=500)\n",
      "Epoch 1/2, Loss: 2.2627\n",
      "Epoch 2/2, Loss: 2.2427\n",
      "✔ City predictions saved → ./Real_LSTM_Result/City2_predictions.csv\n",
      "\n",
      "=== Processing City3 ===\n",
      "Training City3 Zone 1 (features=500)\n",
      "Epoch 1/2, Loss: 2.2515\n",
      "Epoch 2/2, Loss: 2.2252\n",
      "Training City3 Zone 2 (features=500)\n",
      "Epoch 1/2, Loss: 2.2695\n",
      "Epoch 2/2, Loss: 2.2445\n",
      "Training City3 Zone 3 (features=500)\n",
      "Epoch 1/2, Loss: 2.2997\n",
      "Epoch 2/2, Loss: 2.2722\n",
      "✔ City predictions saved → ./Real_LSTM_Result/City3_predictions.csv\n",
      "\n",
      "✅ Finished: City-level predictions generated (national skipped).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import nn, Tensor\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "# -----------------------------\n",
    "# Setup\n",
    "# -----------------------------\n",
    "ms.set_seed(42)\n",
    "np.random.seed(42)\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE)\n",
    "\n",
    "# >>> QUICK FIX KNOB <<<\n",
    "TOP_K = 500   # try 5000 first; if still heavy, use 2000-3000; if you have more RAM, use 10000.\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM Model\n",
    "# -----------------------------\n",
    "class LSTMModel(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Dense(hidden_size, output_size)\n",
    "\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._output_size = output_size\n",
    "\n",
    "    def construct(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class PharmacyDataset:\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            np.array(self.sequences[idx], dtype=np.float32),\n",
    "            np.array(self.targets[idx], dtype=np.float32)\n",
    "        )\n",
    "\n",
    "# -----------------------------\n",
    "# Load Data\n",
    "# -----------------------------\n",
    "def load_data(file_path, global_columns=None):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    data['addeddate'] = pd.to_datetime(\n",
    "        data['addeddate'].astype(str).str.split().str[0],\n",
    "        errors='coerce'\n",
    "    )\n",
    "    data.dropna(subset=['addeddate'], inplace=True)\n",
    "\n",
    "    data['month'] = data['addeddate'].dt.to_period('M')\n",
    "    data['medication_combination'] = data.groupby('Invoice')['name'].transform(\n",
    "        lambda x: '+'.join(sorted(set(x)))\n",
    "    )\n",
    "\n",
    "    grouped = data.groupby(['month', 'medication_combination']).size().unstack(fill_value=0)\n",
    "\n",
    "    if global_columns is not None:\n",
    "        grouped = grouped.reindex(columns=global_columns, fill_value=0)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# -----------------------------\n",
    "# Create sequences\n",
    "# -----------------------------\n",
    "def create_sequences(data, seq_length=3):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data.iloc[i:i + seq_length].values)\n",
    "        y.append(data.iloc[i + seq_length].values)\n",
    "    return X, y\n",
    "\n",
    "# -----------------------------\n",
    "# Train model\n",
    "# -----------------------------\n",
    "def train_model(model, dataset, loss_fn, optimizer, epochs=2):\n",
    "    model.set_train(True)\n",
    "\n",
    "    def forward_fn(x, y):\n",
    "        return loss_fn(model(x), y)\n",
    "\n",
    "    grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        total, steps = 0.0, 0\n",
    "        for xb, yb in dataset.create_tuple_iterator():\n",
    "            xb, yb = Tensor(xb, ms.float32), Tensor(yb, ms.float32)\n",
    "            loss, grads = grad_fn(xb, yb)\n",
    "            optimizer(grads)\n",
    "            total += float(loss.asnumpy())\n",
    "            steps += 1\n",
    "        print(f\"Epoch {e+1}/{epochs}, Loss: {total/max(steps,1):.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregate models (average)\n",
    "# -----------------------------\n",
    "def aggregate_models(models):\n",
    "    base = models[0]\n",
    "    agg = LSTMModel(\n",
    "        base._input_size, base._hidden_size,\n",
    "        base._num_layers, base._output_size\n",
    "    )\n",
    "\n",
    "    param_sets = [{p.name: p.data.asnumpy() for p in m.get_parameters()} for m in models]\n",
    "\n",
    "    for p in agg.get_parameters():\n",
    "        # Average parameter tensors across models\n",
    "        stacked = np.mean([ps[p.name] for ps in param_sets], axis=0)\n",
    "        p.set_data(Tensor(stacked, p.data.dtype))\n",
    "\n",
    "    return agg\n",
    "\n",
    "# -----------------------------\n",
    "# Predict\n",
    "# -----------------------------\n",
    "def predict_trends(model, data, seq_length=3, steps=3):\n",
    "    model.set_train(False)\n",
    "    x = Tensor(data.iloc[-seq_length:].values.astype(np.float32)).expand_dims(0)\n",
    "\n",
    "    preds = []\n",
    "    for _ in range(steps):\n",
    "        out = model(x).asnumpy()\n",
    "        preds.append(out)\n",
    "        x = ms.ops.concat((x[:, 1:, :], Tensor(out.astype(np.float32)).expand_dims(1)), axis=1)\n",
    "\n",
    "    return np.array(preds)\n",
    "\n",
    "# -----------------------------\n",
    "# Build Top-K global columns by frequency\n",
    "# -----------------------------\n",
    "def build_topk_global_columns(input_root, cities, zones_per_city, pharmacies_per_zone, top_k):\n",
    "    # Count frequency of medication combinations across all files\n",
    "    counts = {}\n",
    "\n",
    "    for city in cities:\n",
    "        for z in range(1, zones_per_city + 1):\n",
    "            for p in range(1, pharmacies_per_zone + 1):\n",
    "                path = os.path.join(\n",
    "                    input_root, city, f\"Zone{z}\",\n",
    "                    f\"Ph{p:02d}_Z{z:02d}_C{cities.index(city)+1:02d}.csv\"\n",
    "                )\n",
    "                if not os.path.exists(path):\n",
    "                    continue\n",
    "\n",
    "                d = pd.read_csv(path)\n",
    "                if \"Invoice\" not in d.columns or \"name\" not in d.columns:\n",
    "                    continue\n",
    "\n",
    "                combos = d.groupby(\"Invoice\")[\"name\"].apply(lambda x: '+'.join(sorted(set(x))))\n",
    "                vc = combos.value_counts()\n",
    "\n",
    "                for k, v in vc.items():\n",
    "                    counts[k] = counts.get(k, 0) + int(v)\n",
    "\n",
    "    if not counts:\n",
    "        return []\n",
    "\n",
    "    # Take Top-K most frequent combinations\n",
    "    top = sorted(counts.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    global_cols = [k for k, _ in top]\n",
    "    return global_cols\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_root = \"./outputDateUni/\"\n",
    "    result_root = \"./Real_LSTM_Result/\"\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "\n",
    "    cities = [\"City1\", \"City2\", \"City3\"]\n",
    "    zones_per_city = 3\n",
    "    pharmacies_per_zone = 4\n",
    "    seq_length = 3\n",
    "\n",
    "    print(f\"Building Top-{TOP_K} global columns (this prevents feature explosion)...\")\n",
    "    global_columns = build_topk_global_columns(\n",
    "        input_root, cities, zones_per_city, pharmacies_per_zone, TOP_K\n",
    "    )\n",
    "\n",
    "    if not global_columns:\n",
    "        raise RuntimeError(\"No global columns were built. Check your dataset paths/columns (Invoice/name).\")\n",
    "\n",
    "    print(f\"Using global feature size = {len(global_columns)} columns\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Train zones & aggregate per city\n",
    "    # -----------------------------\n",
    "    for city in cities:\n",
    "        print(f\"\\n=== Processing {city} ===\")\n",
    "        zone_models = []\n",
    "        zone_data = []\n",
    "\n",
    "        for z in range(1, zones_per_city + 1):\n",
    "            dfs = []\n",
    "            for p in range(1, pharmacies_per_zone + 1):\n",
    "                path = os.path.join(\n",
    "                    input_root, city, f\"Zone{z}\",\n",
    "                    f\"Ph{p:02d}_Z{z:02d}_C{cities.index(city)+1:02d}.csv\"\n",
    "                )\n",
    "                if os.path.exists(path):\n",
    "                    dfs.append(load_data(path, global_columns))\n",
    "\n",
    "            if not dfs:\n",
    "                continue\n",
    "\n",
    "            merged = sum(dfs) / len(dfs)\n",
    "\n",
    "            # If still too wide (unlikely now), hard-cap columns (extra safety)\n",
    "            if merged.shape[1] > TOP_K:\n",
    "                merged = merged.iloc[:, :TOP_K]\n",
    "\n",
    "            X, y = create_sequences(merged, seq_length)\n",
    "            if not X:\n",
    "                continue\n",
    "\n",
    "            dataset = GeneratorDataset(\n",
    "                PharmacyDataset(X, y),\n",
    "                [\"x\", \"y\"], shuffle=True\n",
    "            ).batch(32)\n",
    "\n",
    "            model = LSTMModel(\n",
    "                merged.shape[1], 64, 2, merged.shape[1]\n",
    "            )\n",
    "\n",
    "            opt = nn.Adam(model.trainable_params(), learning_rate=0.001)\n",
    "            loss = nn.MSELoss()\n",
    "\n",
    "            print(f\"Training {city} Zone {z} (features={merged.shape[1]})\")\n",
    "            train_model(model, dataset, loss, opt, epochs=2)\n",
    "\n",
    "            zone_models.append(model)\n",
    "            zone_data.append(merged)\n",
    "\n",
    "        # -----------------------------\n",
    "        # City aggregation + City prediction CSV\n",
    "        # -----------------------------\n",
    "        if zone_models:\n",
    "            city_model = aggregate_models(zone_models)\n",
    "            city_data = sum(zone_data) / len(zone_data)\n",
    "\n",
    "            preds = predict_trends(city_model, city_data)\n",
    "            df = pd.DataFrame(preds.squeeze(), columns=city_data.columns)\n",
    "\n",
    "            out_path = os.path.join(result_root, f\"{city}_predictions.csv\")\n",
    "            df.to_csv(out_path, index=False)\n",
    "            print(f\"✔ City predictions saved → {out_path}\")\n",
    "        else:\n",
    "            print(f\"⚠ No trained zones found for {city}. No CSV generated.\")\n",
    "\n",
    "    print(\"\\n✅ Finished: City-level predictions generated (national skipped).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb41fe-ec89-422e-a257-2e7595f208f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MindSpore msenv)",
   "language": "python",
   "name": "msenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
